{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc37bcd-5f4d-45d4-9d99-aac1a375039d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset loading with memory optimizations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analysis progress: : 102663it [01:31, 1124.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up temporary directory /tmp/tf_dataset_e1exi87r\n",
      "Extracting dataset with disk storage to reduce memory usage...\n",
      "Created temporary directory at /tmp/tf_dataset_5cqo1gps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 10:52:53.032657: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Extraction progress: : 102663it [02:51, 597.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset extraction complete. Stored in 11 chunks on disk.\n",
      "Memory usage reduced by keeping only indices and labels in memory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating train dataset from examples: 100%|█| 71842/71842 [00:00<00:00, 6413536.\n",
      "Loading and writing chunks: 100%|███████████████| 11/11 [04:10<00:00, 22.74s/it]\n",
      "Creating val dataset from examples: 100%|█| 15431/15431 [00:00<00:00, 5956953.98\n",
      "Loading and writing chunks: 100%|███████████████| 10/10 [02:17<00:00, 13.79s/it]\n",
      "Creating test dataset from examples: 100%|█| 15390/15390 [00:00<00:00, 2860132.8\n",
      "Loading and writing chunks: 100%|███████████████| 11/11 [01:17<00:00,  7.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying train dataset from /tmp/tf_dataset_5cqo1gps/temp_train_dataset.tfrecord to /home/piotr/Pulpit/codebook/studies/bachelor-thesis/datasets/tfrecord-dataset/nih-dataset/train.tfrecord\n",
      "Copying validation dataset from /tmp/tf_dataset_5cqo1gps/temp_val_dataset.tfrecord to /home/piotr/Pulpit/codebook/studies/bachelor-thesis/datasets/tfrecord-dataset/nih-dataset/val.tfrecord\n",
      "Copying test dataset from /tmp/tf_dataset_5cqo1gps/temp_test_dataset.tfrecord to /home/piotr/Pulpit/codebook/studies/bachelor-thesis/datasets/tfrecord-dataset/nih-dataset/test.tfrecord\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from src.data.tensorflow_data_loader import DataLoader\n",
    "from src.utils.consts import TF_BUFFER_SIZE, TF_BATCH_SIZE\n",
    "\n",
    "def main():\n",
    "    print(\"Starting dataset loading with memory optimizations...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Create DataLoader with reduced buffer size\n",
    "        loader = DataLoader()\n",
    "        loader.get_datasets()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing datasets: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c53086-cdd3-4de2-a685-6344a2cf3fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
