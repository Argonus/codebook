{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301628f-2951-4c66-a27c-4d100a3bc812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Deps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import Locals\n",
    "from src.utils.consts import TF_RECORD_DATASET, TF_BUFFER_SIZE\n",
    "from src.model.tensorflow_utils import load_and_split_dataset\n",
    "from src.model.tensorflow_utils import setup_logger, setup_training_logger, setup_weight_monitor\n",
    "\n",
    "\n",
    "# Input Data\n",
    "DATASET_SIZE = 102697\n",
    "tfrecord_path = f\"{TF_RECORD_DATASET}/chest_xray_data.tfrecord\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c7552-37ca-4191-83b7-67ae747c4861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7405a29c-ce6b-431a-ab38-806013f3bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000\n",
    "tfrecord_buffer_size = TF_BUFFER_SIZE\n",
    "dataset_size = DATASET_SIZE\n",
    "\n",
    "train_ds, val_ds, test_ds = load_and_split_dataset(\n",
    "    tfrecord_path, \n",
    "    batch_size, \n",
    "    shuffle_buffer_size, \n",
    "    tfrecord_buffer_size, \n",
    "    dataset_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a1589e-a284-4875-8dda-5ea89b4521fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DensNet121 like Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Concatenate, AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def conv_block(x, filters, kernel_size=(3,3), strides=1, padding=\"same\"):\n",
    "    \"\"\"\n",
    "    Podstawowa warstwa konwolucyjna z BatchNorm i ReLU\n",
    "    \"\"\"\n",
    "    x = Conv2D(\n",
    "        filters, \n",
    "        kernel_size, s\n",
    "        trides=strides, \n",
    "        padding=padding, \n",
    "        use_bias=False,\n",
    "        # L2 Regularization\n",
    "        kernel_regularizer=l2(1e-4),\n",
    "        kernel_initializer='he_normal'\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    return x\n",
    "\n",
    "def dense_block(x, num_layers, growth_rate):\n",
    "    \"\"\"\n",
    "    Dense Block - łączy wyjścia wielu warstw konwolucyjnych\n",
    "    \"\"\"\n",
    "    concat_features = [x]\n",
    "    for _ in range(num_layers):\n",
    "        conv_out = conv_block(x, growth_rate)\n",
    "        concat_features.append(conv_out)\n",
    "        x = Concatenate()(concat_features)  # Połączenie cech z poprzednich warstw\n",
    "    return x\n",
    "\n",
    "def transition_layer(x, compression=0.5):\n",
    "    \"\"\"\n",
    "    Redukuje liczbę cech po każdym bloku\n",
    "    \"\"\"\n",
    "    filters = int(tf.keras.backend.int_shape(x)[-1] * compression)  # Zmniejsza liczbę kanałów\n",
    "    x = conv_block(x, filters, kernel_size=(1,1))  # Redukcja wymiarów przez 1x1 Conv\n",
    "    x = AveragePooling2D(pool_size=(2,2), strides=2, padding=\"same\")(x)  # Pooling\n",
    "    return x\n",
    "\n",
    "def build_densenet121(num_classes: int) -> Model:\n",
    "    \"\"\"\n",
    "    Implementacja DenseNet121 od podstaw.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(224, 224, 3), name=\"input_layer\")\n",
    "\n",
    "    # Warstwa Początkowa\n",
    "    x = conv_block(inputs, filters=64, kernel_size=(7,7), strides=2, padding=\"same\")\n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=2, padding=\"same\")(x)\n",
    "\n",
    "    # Dense Block 1\n",
    "    x = dense_block(x, num_layers=6, growth_rate=32)\n",
    "    # Transition Layer 1\n",
    "    x = transition_layer(x)                          \n",
    "\n",
    "    # Dense Block 2\n",
    "    x = dense_block(x, num_layers=12, growth_rate=32)\n",
    "    # Transition Layer 2\n",
    "    x = transition_layer(x)                           \n",
    "\n",
    "    # Dense Block 3\n",
    "    x = dense_block(x, num_layers=24, growth_rate=32)\n",
    "    # Transition Layer 3\n",
    "    x = transition_layer(x)                         \n",
    "\n",
    "    # Dense Block 4\n",
    "    x = dense_block(x, num_layers=16, growth_rate=32)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Dropout, żeby uniknąć przeuczenia\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Warstwa wyjściowa (multi-label classification)\n",
    "    outputs = Dense(num_classes, activation=\"sigmoid\", name=\"output_layer\")(x)\n",
    "\n",
    "    # Tworzenie modelu\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"CustomDenseNet121\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Tworzenie modelu dla 14 klas\n",
    "num_classes = 14\n",
    "model = build_densenet121(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6742349e-0a4d-4fba-a8b8-4b0b3d589aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Kompilacja modelu\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss=BinaryCrossentropy(), metrics=[\"accuracy\"])\n",
    "\n",
    "# Checkpoint - zapisuje najlepszy model\n",
    "checkpoint = ModelCheckpoint(\"best_model.h5\", monitor=\"val_loss\", save_best_only=True, mode=\"min\")\n",
    "\n",
    "# ReduceLROnPlateau - zmniejsza LR, gdy walidacja przestaje się poprawiać\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Trenowanie modelu\n",
    "train_ds = train_ds.take(1000)\n",
    "val_ds = val_ds.take(200)\n",
    "test_ds = test_ds.take(100)\n",
    "steps_per_epoch = len(list(train_ds))  # Count number of batches in dataset\n",
    "logger = setup_logger()\n",
    "training_logger = setup_training_logger(logger, 100)\n",
    "weight_monitor = setup_weight_monitor(logger)\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=[checkpoint, reduce_lr, training_logger, weight_monitor]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5c319-0652-41b9-94e5-e9a0fe537450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
