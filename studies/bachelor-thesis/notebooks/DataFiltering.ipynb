{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a39d6d4c-9b66-4328-9489-2fede9ee3407",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.consts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Local Imports\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconsts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAW_DATASET\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconsts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CLEARED_DATASET\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcalculations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m standard_deviation_bounds\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src.consts'"
     ]
    }
   ],
   "source": [
    "# Local Imports\n",
    "from src.utils.consts import RAW_DATASET, CLEARED_DATASET\n",
    "from src.utils.calculations import standard_deviation_bounds\n",
    "\n",
    "from src.data.image_utils import get_image_array\n",
    "from src.data.image_utils import calculate_ssim\n",
    "\n",
    "# Packages Imports\n",
    "import re\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db87f10-40bd-4ec6-a1cf-fc7d53e1eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patients Age Clearing\n",
    "# Reload the dataset\n",
    "data_entry_path = f'{RAW_DATASET}/Data_Entry_2017.csv'\n",
    "data_entry_df = pd.read_csv(data_entry_path, delimiter=',', nrows=None)\n",
    "total_count = len(data_entry_df)\n",
    "\n",
    "# Group by Patient ID and get the first age for each patient\n",
    "unique_patients = data_entry_df.groupby('Patient ID')['Patient Age'].first()\n",
    "\n",
    "# Calculate mean and standard deviation of ages\n",
    "mean_unique_age = unique_patients.mean()\n",
    "std_dev_unique_age = unique_patients.std()\n",
    "\n",
    "# Define outlier thresholds - age\n",
    "outlier_threshold_upper_std = round(mean_unique_age + 3 * std_dev_unique_age, 4)\n",
    "outlier_threshold_lower_std = 0 # Can't be younger than 0 years \n",
    "\n",
    "# Patients in age between [0 and 3Ïƒ]\n",
    "filtered_unique_patients = unique_patients[\n",
    "    (unique_patients >= outlier_threshold_lower_std) & \n",
    "    (unique_patients <= outlier_threshold_upper_std)\n",
    "]\n",
    "\n",
    "filtered_unique_patients = filtered_unique_patients.reset_index()\n",
    "valid_patient_ids = filtered_unique_patients['Patient ID'].tolist()\n",
    "filtered_data = data_entry_df[data_entry_df['Patient ID'].isin(valid_patient_ids)]\n",
    "\n",
    "print(f\"Total Patients: {len(data_entry_df)}\")\n",
    "print(f\"Filtered Patients: {len(filtered_data)}\")\n",
    "print(f\"Total Unique Patients: {len(unique_patients)}\")\n",
    "print(f\"Filtered Unique Parients: {len(filtered_unique_patients)}\")\n",
    "print(f\"Lose Ration: {round(100 - (len(filtered_data) / len(data_entry_df)) * 100, 2)}%\")\n",
    "\n",
    "print(f\"Data Entry Diff: {len(data_entry_df) - len(filtered_data)}\")\n",
    "print(f\"Max Age: {outlier_threshold_upper_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c0a15-395b-4eaf-a639-fb2d42415277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling Clearing\n",
    "filtered_data_copy = filtered_data.copy()\n",
    "total_size = len(filtered_data)\n",
    "\n",
    "dominant_class = \"No Finding\"\n",
    "dominant_class_data = filtered_data_copy[filtered_data_copy['Finding Labels'] == dominant_class]\n",
    "minority_classes_data = filtered_data_copy[filtered_data_copy['Finding Labels'] != dominant_class]\n",
    "\n",
    "assert total_size == len(dominant_class_data) + len(minority_classes_data)\n",
    "\n",
    "undersample_rate = round(len(minority_classes_data) / len(dominant_class_data), 2)\n",
    "undersample_size = int(len(dominant_class_data) * undersample_rate)\n",
    "\n",
    "balanced_dominant_class = dominant_class_data.sample(n=undersample_size, random_state=42)\n",
    "balanced_data = pd.concat([balanced_dominant_class, minority_classes_data], ignore_index=True)\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total Patients: {len(filtered_data_copy)}\")\n",
    "print(f\"Filtered Patients: {len(balanced_data)}\")\n",
    "print(f\"Lose Ration: {round(100 - (len(balanced_data) / len(filtered_data_copy)) * 100, 2)}%\")\n",
    "\n",
    "print(f\"Total Lose: {(len(data_entry_df) - len(balanced_data))}\")\n",
    "print(f\"Total Lose Ration: {round(100 - (len(balanced_data) / len(data_entry_df)) * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed2c31-b05d-480f-ae2c-a1400971b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in cleared Dataset\n",
    "cleared_data_enetry = f'{CLEARED_DATASET}/Data_Entry_2017.csv'\n",
    "balanced_data.to_csv(cleared_data_enetry, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e0becf-60bc-40a4-82f1-0d6051447b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Bounding Box Data\n",
    "data_entry_file = f'{RAW_DATASET}/BBox_List_2017.csv'\n",
    "bbox_data_df = pd.read_csv(data_entry_file, delimiter=',', nrows=None)\n",
    "\n",
    "renamed_bbox_data = bbox_data_df.dropna(axis=1, how='all').copy()\n",
    "renamed_bbox_data.rename(columns={\"Bbox [x\": \"x\", \"h]\": \"h\"}, inplace=True)\n",
    "\n",
    "# Calculage widh & heigh data bounds\n",
    "mean_w = renamed_bbox_data['w'].mean()\n",
    "std_w = renamed_bbox_data['w'].std()\n",
    "_, w_outlier_threshold = standard_deviation_bounds(mean_w, std_w, 3)\n",
    "\n",
    "mean_h = renamed_bbox_data['h'].mean()\n",
    "std_h = renamed_bbox_data['h'].std()\n",
    "_lower_bound, h_outlier_threshold = standard_deviation_bounds(mean_h, std_h, 3)\n",
    "\n",
    "\n",
    "filtered_bbox_data = renamed_bbox_data[\n",
    "    (renamed_bbox_data['w'] <= w_outlier_threshold) & \n",
    "    (renamed_bbox_data['h'] <= h_outlier_threshold)\n",
    "]\n",
    "\n",
    "print(f\"W: {w_outlier_threshold}, H: {h_outlier_threshold}\")\n",
    "print(f\"Original dataset size: {len(renamed_bbox_data)}\")\n",
    "print(f\"Filtered dataset size: {len(filtered_bbox_data)}\")\n",
    "print(f\"Data lose: {len(renamed_bbox_data) - len(filtered_bbox_data)}\")\n",
    "print(f\"Data lose ration: {round(100 - (len(filtered_bbox_data) / len(renamed_bbox_data)) * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17064e4-f306-46c2-b99f-30327932c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in cleared Dataset\n",
    "cleared_data_enetry = f'{CLEARED_DATASET}/BBox_List_2017.csv'\n",
    "filtered_bbox_data.to_csv(cleared_data_enetry, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6f76d-5029-457b-95e0-905383786dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images Data Clearing\n",
    "pattern = re.compile(r'images_\\d{3}')\n",
    "matching_dirs = [\n",
    "    os.path.join(RAW_DATASET, d)\n",
    "    for d in os.listdir(RAW_DATASET)\n",
    "    if os.path.isdir(os.path.join(RAW_DATASET, d)) and pattern.match(d)\n",
    "]\n",
    "\n",
    "image_files = set()\n",
    "for directory in matching_dirs:\n",
    "    nested_dir = f\"{directory}/images\"\n",
    "    image_files_in_dir = os.listdir(nested_dir)\n",
    "    for image in image_files_in_dir:\n",
    "        image_path = os.path.join(nested_dir, image)\n",
    "        image_files.add(image_path)\n",
    "\n",
    "images_df = pd.DataFrame(list(image_files))\n",
    "images_df.columns = ['image_path']\n",
    "images_df['intensity'] = None\n",
    "images_df['ssim'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737aabc6-135b-4174-9fdb-5f43f0340556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Image Intensity\n",
    "for index, row in tqdm(images_df.iterrows(), total=len(images_df)):\n",
    "    image_path = row['image_path']\n",
    "    img_array = get_image_array(image_path)\n",
    "    intensity = img_array.mean()\n",
    "    images_df.at[index, 'intensity'] = intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac41b94-d1c4-40c0-ade8-8d97898ae04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter By Intensity\n",
    "mean_intensity = np.mean(images_df['intensity'])\n",
    "std_intensity = np.std(images_df['intensity'])\n",
    "lower_bound, upper_bound = standard_deviation_bounds(mean_intensity, std_intensity, 3)\n",
    "filtered_images_df = images_df[(images_df['intensity'] >= lower_bound) & (images_df['intensity'] <= upper_bound)]\n",
    "\n",
    "print(\"Upper Outlier Threshold:\", round(upper_bound, 2))\n",
    "print(\"Lower Outlier Threshold:\", round(lower_bound, 2))\n",
    "print(f\"Final dataset size before removing outliers: {len(images_df)}\")\n",
    "print(f\"Final dataset size after removing outliers: {len(filtered_images_df)}\")\n",
    "print(f\"Dataset size diff: {len(images_df) - len(filtered_images_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2225cd7-b013-4ad5-accc-9efea596ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "import traceback\n",
    "\n",
    "# Compute median intensity\n",
    "median_intensity = np.median(filtered_images_df['intensity'])\n",
    "print(f\"Median pixel intensity: {median_intensity:.2f}\")\n",
    "\n",
    "closest_image_index = np.argmin([abs(intensity - median_intensity) for intensity in filtered_images_df['intensity']])\n",
    "print(f\"Closest image index: {closest_image_index}\")\n",
    "print(f\"Closest image intensity: {filtered_images_df.iloc[closest_image_index]['intensity']}\")\n",
    "\n",
    "reference_image_path = filtered_images_df.iloc[closest_image_index]['image_path']\n",
    "reference_image = get_image_array(image_path)\n",
    "\n",
    "# TODO: Process images concurrently.\n",
    "for index, row in tqdm(filtered_images_df.iterrows(), total=len(filtered_images_df)):\n",
    "    try:\n",
    "        comparison_image = get_image_array(row['image_path'])\n",
    "        ssim_value = calculate_ssim(comparison_image, reference_image)\n",
    "        filtered_images_df.at[index, 'ssim'] = ssim_value\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"BÅ‚Ä…d przetwarzania {image_path}: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "print(filtered_images_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c8123-386e-4304-8751-2fb842603035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and standard deviation\n",
    "mean_ssim = np.mean(filtered_images_df['ssim'])\n",
    "std_ssim = np.std(filtered_images_df['ssim'])\n",
    "\n",
    "lower_bound, upper_bound = standard_deviation_bounds(mean_ssim, std_ssim, 3)\n",
    "print(\"Upper Outlier Threshold:\", round(upper_bound, 4))\n",
    "print(\"Lower Outlier Threshold:\", round(lower_bound, 4))\n",
    "\n",
    "simm_images_df = filtered_images_df[\n",
    "    (filtered_images_df['ssim'] <= upper_bound) &  # Inside upper limit\n",
    "    (filtered_images_df['ssim'] >= lower_bound)    # Inside lower limit\n",
    "]\n",
    "\n",
    "print(f\"Final dataset size before removing outliers: {len(filtered_images_df)}\")\n",
    "print(f\"Final dataset size after removing outliers: {len(simm_images_df)}\")\n",
    "print(f\"Dataset size diff: {len(filtered_images_df) - len(simm_images_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf8420c-f531-498e-82f5-f2709ac6be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload data entry\n",
    "clean_data_entry_df = pd.read_csv(f\"{CLEARED_DATASET}/Data_Entry_2017.csv\")\n",
    "\n",
    "# Extract Image Index from images_path (assuming file name is the last part of the path)\n",
    "simm_images_df[\"Image Index\"] = simm_images_df[\"image_path\"].apply(lambda x: os.path.basename(x))\n",
    "simm_images_df[\"Image Index\"] = simm_images_df[\"Image Index\"].str.strip()\n",
    "\n",
    "# Filter simm_images_df to keep only existing images in DataEntry\n",
    "existing_images_df = simm_images_df[simm_images_df[\"Image Index\"].isin(clean_data_entry_df[\"Image Index\"])]\n",
    "\n",
    "print(f\"Final dataset size before removing outliers: {len(simm_images_df)}\")\n",
    "print(f\"Final dataset size after removing outliers: {len(existing_images_df)}\")\n",
    "print(f\"Dataset size diff: {len(simm_images_df) - len(existing_images_df)}\")\n",
    "print(f\"Init dataset size: {len(images_df)}\")\n",
    "\n",
    "print(f\"Lose Ration: {round(100 - (len(existing_images_df) / len(images_df)) * 100, 4)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c80e8-76b0-44e2-bcae-382073f8f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleared data copy\n",
    "import shutil\n",
    "\n",
    "# Define source and destination folders\n",
    "images_dataset = f\"{CLEARED_DATASET}/images\"\n",
    "os.makedirs(images_dataset, exist_ok=True)\n",
    "\n",
    "for index, row in tqdm(existing_images_df.iterrows(), total=len(existing_images_df)):\n",
    "    try:\n",
    "        image_path = row['image_path']\n",
    "        source_path = os.path.abspath(image_path)  # If paths are absolute in the DataFrame\n",
    "        destination_path = os.path.join(images_dataset, os.path.basename(image_path))\n",
    "\n",
    "        if os.path.exists(source_path):\n",
    "            shutil.copy2(source_path, images_dataset)\n",
    "        else:\n",
    "            tqdm.warning(f\"Warning: File not found - {source_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying {image_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd44fd3-d53a-4351-abca-f65e3ddc0d24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
