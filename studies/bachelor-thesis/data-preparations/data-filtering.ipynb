{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6bc1709-f52b-4311-9dff-25e27e42c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "nih_dataset = '../data/nih-dataset'\n",
    "cleared_dataset = '../cleared-data/nih-dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64fab830-6c93-4281-90e9-c3025a52b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patients Age Clearing\n",
    "# Reload the dataset\n",
    "data_entry_file = f'{nih_dataset}/Data_Entry_2017.csv'\n",
    "data_frame = pd.read_csv(data_entry_file, delimiter=',', nrows=None)\n",
    "\n",
    "total_count = len(data_frame)\n",
    "\n",
    "# Group by Patient ID and get the first age for each patient\n",
    "unique_patients = data_frame.groupby('Patient ID')['Patient Age'].first()\n",
    "\n",
    "# Calculate mean and standard deviation of ages\n",
    "mean_unique_age = unique_patients.mean()\n",
    "std_dev_unique_age = unique_patients.std()\n",
    "\n",
    "# Define outlier thresholds - age\n",
    "outlier_threshold_upper_std = round(mean_unique_age + 3 * std_dev_unique_age, 4)\n",
    "outlier_threshold_lower_std = 0 # Can't be younger than 0 years \n",
    "\n",
    "# Patients in age between [0 and 3σ]\n",
    "filtered_unique_patients = unique_patients[\n",
    "    (unique_patients >= outlier_threshold_lower_std) & \n",
    "    (unique_patients <= outlier_threshold_upper_std)\n",
    "]\n",
    "\n",
    "filtered_unique_patients = filtered_unique_patients.reset_index()\n",
    "valid_patient_ids = filtered_unique_patients['Patient ID'].tolist()\n",
    "filtered_data = data_frame[data_frame['Patient ID'].isin(valid_patient_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b84138dc-5394-4bc9-9a1a-e99b86213e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling Clearing\n",
    "# Reload the dataset\n",
    "total_size = len(filtered_data)\n",
    "\n",
    "dominant_class = \"No Finding\"\n",
    "dominant_class_data = filtered_data[filtered_data['Finding Labels'] == dominant_class]\n",
    "minority_classes_data = filtered_data[filtered_data['Finding Labels'] != dominant_class]\n",
    "\n",
    "assert total_size == len(dominant_class_data) + len(minority_classes_data)\n",
    "\n",
    "undersample_rate = round(len(minority_classes_data) / len(dominant_class_data), 2)\n",
    "undersample_size = int(len(dominant_class_data) * undersample_rate)\n",
    "\n",
    "balanced_dominant_class = dominant_class_data.sample(n=undersample_size, random_state=42)\n",
    "balanced_data = pd.concat([balanced_dominant_class, minority_classes_data], ignore_index=True)\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5514460-a41c-42fa-905e-8fc6375d1129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in cleated Dataset\n",
    "cleared_data_enetry = f'{cleared_dataset}/Data_Entry_2017.csv'\n",
    "balanced_data.to_csv(cleared_data_enetry, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad15c1a8-7511-4a67-8a35-c376e9a9a402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: 759.22, H: 730.63\n",
      "Original dataset size: 985\n",
      "Filtered dataset size: 973\n",
      "Data lose: 12\n",
      "Data lose ration: 1.22 \n"
     ]
    }
   ],
   "source": [
    "# Prepare Bounding Box Data\n",
    "data_entry_file = f'{nih_dataset}/BBox_List_2017.csv'\n",
    "bbox_data = pd.read_csv(data_entry_file, delimiter=',', nrows=None)\n",
    "renamed_bbox_data = bbox_data.dropna(axis=1, how='all').copy()\n",
    "renamed_bbox_data.rename(columns={\"Bbox [x\": \"x\", \"h]\": \"h\"}, inplace=True)\n",
    "\n",
    "# Calculage widh & heigh data bounds\n",
    "mean_w = renamed_bbox_data['w'].mean()\n",
    "std_w = renamed_bbox_data['w'].std()\n",
    "mean_h = renamed_bbox_data['h'].mean()\n",
    "std_h = renamed_bbox_data['h'].std()\n",
    "w_outlier_threshold = round(mean_w + 3 * std_w, 2)\n",
    "h_outlier_threshold = round(mean_h + 3 * std_h, 2)\n",
    "\n",
    "print(f\"W: {w_outlier_threshold}, H: {h_outlier_threshold}\")\n",
    "\n",
    "filtered_bbox_data = renamed_bbox_data[\n",
    "    (renamed_bbox_data['w'] <= w_outlier_threshold) & \n",
    "    (renamed_bbox_data['h'] <= h_outlier_threshold)\n",
    "]\n",
    "\n",
    "print(f\"Original dataset size: {len(renamed_bbox_data)}\")\n",
    "print(f\"Filtered dataset size: {len(filtered_bbox_data)}\")\n",
    "print(f\"Data lose: {len(renamed_bbox_data) - len(filtered_bbox_data)}\")\n",
    "print(f\"Data lose ration: {round(100 - (len(filtered_bbox_data) / len(renamed_bbox_data)) * 100,2)} \")\n",
    "\n",
    "# Store in cleated Dataset\n",
    "cleared_data_enetry = f'{cleared_dataset}/BBox_List_2017.csv'\n",
    "filtered_bbox_data.to_csv(cleared_data_enetry, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c9ae87-9011-491f-8549-9a6a950e0489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          image_path intensity  ssim\n",
      "0  ../data/nih-dataset/images_008/images/00016990...      None  None\n",
      "1  ../data/nih-dataset/images_005/images/00011101...      None  None\n",
      "2  ../data/nih-dataset/images_003/images/00005515...      None  None\n",
      "3  ../data/nih-dataset/images_010/images/00022822...      None  None\n",
      "4  ../data/nih-dataset/images_010/images/00021264...      None  None\n"
     ]
    }
   ],
   "source": [
    "# Images Data Clearing\n",
    "from PIL import Image\n",
    "import re\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "pattern = re.compile(r'images_\\d{3}')\n",
    "matching_dirs = [\n",
    "    os.path.join(nih_dataset, d)\n",
    "    for d in os.listdir(nih_dataset)\n",
    "    if os.path.isdir(os.path.join(nih_dataset, d)) and pattern.match(d)\n",
    "]\n",
    "\n",
    "image_files = set()\n",
    "for directory in matching_dirs:\n",
    "    nested_dir = f\"{directory}/images\"\n",
    "    image_files_in_dir = os.listdir(nested_dir)\n",
    "    for image in image_files_in_dir:\n",
    "        image_path = os.path.join(nested_dir, image)\n",
    "        image_files.add(image_path)\n",
    "\n",
    "images_df = pd.DataFrame(list(image_files))\n",
    "images_df.columns = ['image_path']\n",
    "images_df['intensity'] = None\n",
    "images_df['ssim'] = None\n",
    "print(images_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a204f438-5dbf-41a5-8980-5341b73cd84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▏                                                                                                                                                                                                                                                                     | 904/112120 [00:06<12:55, 143.43it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m     images_df\u001b[38;5;241m.\u001b[39mat[index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintensity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_pixel_intensity_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     15\u001b[0m     tqdm\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m, in \u001b[0;36mcalculate_pixel_intensity_stats\u001b[0;34m(image_file)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mopen(image_file) \u001b[38;5;28;01mas\u001b[39;00m img:\n\u001b[1;32m      5\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     img_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Convert to NumPy array\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img_array\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Filter by intensity\n",
    "\n",
    "def calculate_pixel_intensity_stats(image_file):\n",
    "    with Image.open(image_file) as img:\n",
    "        img = img.convert('L')\n",
    "        img_array = np.array(img, dtype=np.float32)  # Convert to NumPy array\n",
    "        return img_array.mean()\n",
    "            \n",
    "intensities = []\n",
    "for index, row in tqdm(images_df.iterrows(), total=len(images_df)):\n",
    "    try:\n",
    "        image_path = row['image_path']\n",
    "        images_df.at[index, 'intensity'] = calculate_pixel_intensity_stats(image_path)\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"Error processing {image}: {e}\")\n",
    "        continue\n",
    "\n",
    "mean_intensity = np.mean(images_df['intensity'])\n",
    "std_intensity = np.std(images_df['intensity'])\n",
    "\n",
    "mean_intensity_upper_outlier = round(mean_intensity + 3 * std_intensity, 4)\n",
    "mean_intensity_lower_outlier = round(mean_intensity - 3 * std_intensity, 4)\n",
    "\n",
    "print(\"Upper Outlier Threshold:\", mean_intensity_upper_outlier)\n",
    "print(\"Lower Outlier Threshold:\", mean_intensity_lower_outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ebe60-6e58-4100-849f-dd5a9ef68ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_images_df = images_df[\n",
    "    (images_df['intensity'] >= mean_intensity_lower_outlier) &\n",
    "    (images_df['intensity'] <= mean_intensity_upper_outlier)\n",
    "]\n",
    "\n",
    "print(f\"Final dataset size before removing outliers: {len(images_df)}\")\n",
    "print(f\"Final dataset size after removing outliers: {len(filtered_images_df)}\")\n",
    "print(f\"Dataset size diff: {len(images_df) - len(filtered_images_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234d64c6-a7d3-4ae4-bbe6-67e750ae7fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "median_intensity = np.median(filtered_images_df['intensity'])\n",
    "closest_image_index = np.argmin([abs(intensity - median_intensity) for intensity in filtered_images_df['intensity']])\n",
    "\n",
    "print(f\"Median intensity: {median_intensity}\")\n",
    "print(f\"Closest image index: {closest_image_index}\")\n",
    "print(f\"Closest image intensity: {filtered_images_df.iloc[closest_image_index]['intensity']}\")\n",
    "\n",
    "reference_image_path = filtered_images_df.iloc[closest_image_index]['images_path']\n",
    "reference_image = Image.open(reference_image_path).convert(\"L\")\n",
    "reference_image = np.array(reference_image)  # Konwersja do NumPy\n",
    "\n",
    "for index, row in tqdm(filtered_images_df.iterrows(), total=len(filtered_images_df)):\n",
    "    try:\n",
    "        comparison_image = Image.open(row['images_path']).convert(\"L\")\n",
    "        comparison_image = np.array(comparison_image)\n",
    "\n",
    "        if reference_image.shape != comparison_image.shape:\n",
    "            comparison_image = Image.fromarray(comparison_image).resize(\n",
    "                (reference_image.shape[1], reference_image.shape[0]), Image.LANCZOS\n",
    "            )\n",
    "            comparison_image = np.array(comparison_image)\n",
    "\n",
    "        ssim_value, _ = ssim(reference_image, comparison_image, full=True)\n",
    "        filtered_images_df.at[index, 'ssim'] = ssim_value\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Błąd przetwarzania {image_path}: {e}\")\n",
    "\n",
    "print(filtered_images_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263ac67b-97bb-4be6-b3f6-bc59c9a4e3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper Outlier Threshold: 0.8236\n",
      "Lower Outlier Threshold: 0.4617\n",
      "Final dataset size before removing outliers: 111822\n",
      "Final dataset size after removing outliers: 110999\n",
      "Dataset size diff: 823\n"
     ]
    }
   ],
   "source": [
    "# Ensure SSIM is numeric\n",
    "filtered_images_df = pd.read_csv('out.csv')\n",
    "filtered_images_df['ssim'] = pd.to_numeric(filtered_images_df['ssim'], errors='coerce')\n",
    "\n",
    "# Compute mean and standard deviation\n",
    "mean_ssim = np.mean(filtered_images_df['ssim'])\n",
    "std_ssim = np.std(filtered_images_df['ssim'])\n",
    "\n",
    "# Define outlier thresholds\n",
    "mean_ssim_upper_outlier = round(mean_ssim + 3 * std_ssim, 4)\n",
    "mean_ssim_lower_outlier = round(mean_ssim - 3 * std_ssim, 4)\n",
    "\n",
    "print(\"Upper Outlier Threshold:\", mean_ssim_upper_outlier)\n",
    "print(\"Lower Outlier Threshold:\", mean_ssim_lower_outlier)\n",
    "\n",
    "# Corrected filtering condition (inside the range, not outside)\n",
    "simm_images_df = filtered_images_df[\n",
    "    (filtered_images_df['ssim'] <= mean_ssim_upper_outlier) &  # Inside upper limit\n",
    "    (filtered_images_df['ssim'] >= mean_ssim_lower_outlier)    # Inside lower limit\n",
    "]\n",
    "\n",
    "print(f\"Final dataset size before removing outliers: {len(filtered_images_df)}\")\n",
    "print(f\"Final dataset size after removing outliers: {len(simm_images_df)}\")\n",
    "print(f\"Dataset size diff: {len(filtered_images_df) - len(simm_images_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d60e5bf-75b2-444b-b74b-1fb6444457bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         images_path   intensity      ssim  \\\n",
      "0  ../data/nih-dataset/images_003/images/00004402...  127.868210  0.622076   \n",
      "1  ../data/nih-dataset/images_002/images/00003857...  140.788450  0.737914   \n",
      "2  ../data/nih-dataset/images_011/images/00025521...  110.631065  0.591775   \n",
      "3  ../data/nih-dataset/images_003/images/00005567...  121.155815  0.653625   \n",
      "4  ../data/nih-dataset/images_012/images/00030786...  111.963640  0.627443   \n",
      "\n",
      "        Image Index  \n",
      "0  00004402_005.png  \n",
      "1  00003857_001.png  \n",
      "2  00025521_003.png  \n",
      "3  00005567_004.png  \n",
      "4  00030786_007.png  \n",
      "Final dataset size before removing outliers: 110999\n",
      "Final dataset size after removing outliers: 102569\n",
      "Dataset size diff: 8430\n",
      "Init dataset size: 112120\n",
      "Lose Ration: 8.5186%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6d/qkd7tdrx14g83dnp3tqkdd4r0000gp/T/ipykernel_50373/1774687326.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  simm_images_df[\"Image Index\"] = simm_images_df[\"images_path\"].apply(lambda x: os.path.basename(x))\n",
      "/var/folders/6d/qkd7tdrx14g83dnp3tqkdd4r0000gp/T/ipykernel_50373/1774687326.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  simm_images_df[\"Image Index\"] = simm_images_df[\"Image Index\"].str.strip()\n"
     ]
    }
   ],
   "source": [
    "# Filter By Data Entry\n",
    "data_entry_df = pd.read_csv(f\"{cleared_dataset}/Data_Entry_2017.csv\")\n",
    "\n",
    "import os\n",
    "\n",
    "# Extract Image Index from images_path (assuming file name is the last part of the path)\n",
    "simm_images_df[\"Image Index\"] = simm_images_df[\"images_path\"].apply(lambda x: os.path.basename(x))\n",
    "\n",
    "# Ensure Image Index is clean and does not contain any unexpected characters\n",
    "simm_images_df[\"Image Index\"] = simm_images_df[\"Image Index\"].str.strip()\n",
    "print(simm_images_df.head())\n",
    "\n",
    "# Filter simm_images_df to keep only existing images in DataEntry\n",
    "existing_images_df = simm_images_df[simm_images_df[\"Image Index\"].isin(data_entry_df[\"Image Index\"])]\n",
    "\n",
    "print(f\"Final dataset size before removing outliers: {len(simm_images_df)}\")\n",
    "print(f\"Final dataset size after removing outliers: {len(existing_images_df)}\")\n",
    "print(f\"Dataset size diff: {len(simm_images_df) - len(existing_images_df)}\")\n",
    "print(f\"Init dataset size: {len(images_df)}\")\n",
    "\n",
    "print(f\"Lose Ration: {round(100 - (len(existing_images_df) / len(images_df)) * 100, 4)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc7a3561-8bc7-47ce-9dc0-83df24b0f690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 102569/102569 [01:53<00:00, 901.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Save cleared data copy\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define source and destination folders\n",
    "images_dataset = f\"{cleared_dataset}/images\"\n",
    "os.makedirs(images_dataset, exist_ok=True)\n",
    "\n",
    "for index, row in tqdm(existing_images_df.iterrows(), total=len(existing_images_df)):\n",
    "    try:\n",
    "        image_path = row['images_path']\n",
    "        source_path = os.path.abspath(image_path)  # If paths are absolute in the DataFrame\n",
    "        destination_path = os.path.join(images_dataset, os.path.basename(image_path))\n",
    "\n",
    "        if os.path.exists(source_path):\n",
    "            shutil.copy2(source_path, images_dataset)  # Preserve metadata\n",
    "        else:\n",
    "            tqdm.warning(f\"Warning: File not found - {source_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying {image_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09111e7a-e17d-4f33-a036-13515d426a87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
